{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fashion_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1yoNr_XuBXevW-Fu7kF5H2dzRVJ-Yfhsg",
      "authorship_tag": "ABX9TyOdosQUx56twan6f/hHevlQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjay-thiyagarajan/projects/blob/master/fashion_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yygGLDtyb8ml"
      },
      "source": [
        "import tensorflow as tf #Google's Machine Learning Framework\n",
        "from tensorflow import keras #Deep Learning Library under TensorFlow\n",
        "from keras import Sequential #Sequential stack form for modelling\n",
        "from keras.layers import Dense #Dense layers for implementing Hidden Layers in the Neural Network\n",
        "import numpy as np #Library for numerical conversions and calculations\n",
        "import matplotlib.pyplot as plt # Library for data visualization\n",
        "from sklearn import metrics #Evaluation metrics for the model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RErEN00SeDm0",
        "outputId": "aadb5e22-84c9-4233-e9ba-8c77d4f084bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "fashion_mnists = keras.datasets.fashion_mnist #MNIST Fashion Dataset is downloaded and then saved to the dataframe fashion_mnists\n",
        "(train_imgs,train_labels),(test_imgs,test_labels) = fashion_mnists.load_data() #The dataframe is segmented into appropriate training and test sets.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEVPhUEWl9b6",
        "outputId": "2db47be0-b708-4f3e-cfc1-ceac6dfda0c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.imshow(train_imgs[6])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff43374a240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVGy7-LpmJX5"
      },
      "source": [
        "train_imgs = train_imgs / 255.0 #Pixel Resizing for training set\n",
        "test_imgs = test_imgs / 255.0 #Pixel Resizing for test set\n",
        "train_imgs = train_imgs.reshape(60000,28,28,1) #Training data is reshaped into 28x28 dimension and flatttened\n",
        "test_imgs = test_imgs.reshape(10000,28,28,1) #Test data is reshaped into 28x28 dimension and flattened"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIRcRfL5mp5w",
        "outputId": "bae693b2-29fc-45d1-fd4d-b50b4f8ed4c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = Sequential([ #model is attached to a Sequential stack\n",
        "                    keras.layers.Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1)), #64 convolutional filters are applied at the first stage\n",
        "                    keras.layers.MaxPool2D(2,2), #The image is then filtered to half the existing dimension \n",
        "                    keras.layers.Conv2D(64,(3,3),activation='relu'), #image is then again passed through 64 convolutional filters but with filtered features\n",
        "                    keras.layers.MaxPool2D(2,2),#The image is again filtered to half the existing dimensions\n",
        "                    keras.layers.Flatten(), #The feature is then flattened to push through the hidden layers\n",
        "                    Dense(512, activation=tf.nn.relu), #Hidden layer with 512 neurons\n",
        "                    Dense(10, activation=tf.nn.softmax) #Hidden layer with 10 neurons for classification purpose\n",
        "])\n",
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy']) #Uses Adam optimizer and Sparse categorical Cross entropy\n",
        "model.fit(train_imgs,train_labels, epochs=5) #5 epoch training is initiated"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3028 - accuracy: 0.0985\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 2.3028 - accuracy: 0.0981\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3027 - accuracy: 0.0979\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3028 - accuracy: 0.0987\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3027 - accuracy: 0.0997\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7a2805ddd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7mOenEfo0Ba"
      },
      "source": [
        "model.save('/content/drive/My Drive/models/trained_model.h5') #Trained model is then saved "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJb2aY96qtG_",
        "outputId": "af48b931-3251-48e8-887a-8018bb50a2fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.evaluate(test_imgs,test_labels) #Model is evaluated with Loss and Accuracy as parameters\n",
        "pred =  model.predict(test_imgs)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 2.3027 - accuracy: 0.1000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}